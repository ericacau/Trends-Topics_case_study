{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5d2ec163",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Reddit EC detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd16e26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "import operator\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "from cdlib import AttrNodeClustering, evaluation, TemporalClustering\n",
    "from Eva import eva_best_partition, modularity, purity\n",
    "from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.lines as mlines\n",
    "sns.set_palette(\"Set2\")\n",
    "\n",
    "sns.set_style(style='white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "473b42ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_json(file):\n",
    "    '''Load a json file'''\n",
    "    with open(file) as f:\n",
    "        json_data = json.loads(f.read())\n",
    "    return json_data\n",
    "\n",
    "def read_net(filename):\n",
    "    '''Read a network from a csv file'''\n",
    "    print(\"\\nEdgelist: \", filename)\n",
    "    g = nx.Graph()\n",
    "    with open(filename) as f:\n",
    "        f.readline()\n",
    "        for l in f:\n",
    "            l = l.split(\",\")\n",
    "            g.add_edge(l[0], l[1], weight=int(l[2]))\n",
    "    return g\n",
    "\n",
    "def read_labels(filename):\n",
    "    '''Read labels from a csv file'''\n",
    "    node_to_label = {}\n",
    "    with open(filename) as f:\n",
    "        f.readline()\n",
    "        for l in f:\n",
    "            l = l.rstrip().split(\",\")\n",
    "            node_to_label[l[0]] = l[2]\n",
    "    return node_to_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cea6450e",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_results = \"../results/reddit_results/\"\n",
    "text_data = \"../text_data/\"\n",
    "\n",
    "categories = [\"guncontrol\", 'minority', \"politics\"]\n",
    "semesters = [('01/01/2017','01/07/2017'), ('01/07/2017','01/01/2018'),\n",
    "             ('01/01/2018','01/07/2018'), ('01/07/2018','01/01/2019'), ('01/01/2019','01/07/2019')]\n",
    "\n",
    "mods = load_json(\"../data/moderators.json\")\n",
    "bots = load_json(\"../data/bots_reddit.json\")\n",
    "\n",
    "mods_bots_list = list(mods.keys())\n",
    "mods_bots_list += list(bots.keys())\n",
    "\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0b32d40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_labels(g):\n",
    "    comps_list = list(nx.connected_components(g))\n",
    "    max_len = sorted([[len(el),el] for el in comps_list], reverse=True)\n",
    "    comp_0 = nx.subgraph(g, max_len[0][1])\n",
    "    mapping = dict(zip(comp_0, range(0, len(comp_0))))\n",
    "    relabel_comp_0 = nx.relabel_nodes(comp_0, mapping)\n",
    "    inv_map = {v: k for k, v in mapping.items()}\n",
    "    return relabel_comp_0, inv_map\n",
    "\n",
    "\n",
    "def purity_func(coms, labels):\n",
    "    count_coms = defaultdict(int)\n",
    "    purities = []\n",
    "    top_label = []\n",
    "    for n, c in coms.items():\n",
    "        count_coms[c] += 1\n",
    "    for el in labels: \n",
    "        max_label = max(labels[el][\"leaning\"].items(), key = operator.itemgetter(1))[0]\n",
    "        max_label_val = labels[el][\"leaning\"][max_label]\n",
    "        purity = max_label_val/count_coms[el]\n",
    "        purities.append(purity)\n",
    "        top_label.append(max_label)\n",
    "    return purities, top_label\n",
    "\n",
    "\n",
    "def cd_eva(node_label):\n",
    "    coms, com_labels = eva_best_partition(node_label, alpha=0.5)\n",
    "    return coms, com_labels\n",
    "\n",
    "\n",
    "def extract_EC(categories):\n",
    "    matches_dict = defaultdict(list)\n",
    "    for topic in tqdm(categories):\n",
    "        tc = TemporalClustering()\n",
    "        sem = 0\n",
    "        data_path = f\"../data/topic_networks/{topic}/\"\n",
    "        for semester in semesters:\n",
    "            period0_nodelist= datetime.datetime.strptime(semester[0], \"%d/%m/%Y\").strftime(\"%d-%m-%Y\")\n",
    "            period1_nodelist = datetime.datetime.strptime(semester[1], \"%d/%m/%Y\").strftime(\"%d-%m-%Y\")\n",
    "            period0_labels= datetime.datetime.strptime(semester[0], \"%d/%m/%Y\").strftime(\"%Y-%m-%d\")\n",
    "            period1_labels = datetime.datetime.strptime(semester[1], \"%d/%m/%Y\").strftime(\"%Y-%m-%d\")\n",
    "            semester_edgelist = os.path.join(data_path, f'{topic}_{period0_nodelist}_{period1_nodelist}_complete.csv') \n",
    "            semester_labels = os.path.join(data_path, f'{topic}_{period0_labels}_{period1_labels}_labels.csv') \n",
    "            # read the network and add the attributes\n",
    "            g = read_net(semester_edgelist)\n",
    "            nth = read_labels(semester_labels) \n",
    "            nx.set_node_attributes(g, nth, \"leaning\")\n",
    "            nx.set_node_attributes(g, sem, \"snapshot_id\")\n",
    "            \n",
    "            # remove nodes without attributes, mods and bots from the network\n",
    "            nodes_to_remove = list()\n",
    "            for node in g.nodes:\n",
    "                node_dict = g.nodes[node]\n",
    "                if node_dict.get('leaning') is None:\n",
    "                    nodes_to_remove.append(node)\n",
    "                if node in mods_bots_list:\n",
    "                    nodes_to_remove.append(node)\n",
    "            g.remove_nodes_from(nodes_to_remove)\n",
    "\n",
    "            relabel, mapping = map_labels(g)\n",
    "            # community detection with EVA\n",
    "            coms, labels = cd_eva(relabel)\n",
    "            coms_to_node = defaultdict(list)\n",
    "            for n, c in coms.items():\n",
    "                coms_to_node[c].append(n) \n",
    "            coms_eva = [list(c) for c in coms_to_node.values()]\n",
    "            eva_attr_node_clustering = AttrNodeClustering(coms_eva, relabel, \"Eva\", labels, method_parameters={\"weight\": 'weight', \"resolution\": 1,\n",
    "                                                                             \"randomize\": False, \"alpha\":0.5})\n",
    "            tc.add_clustering(eva_attr_node_clustering, sem)\n",
    "            eva_comm = list()\n",
    "            for com in eva_attr_node_clustering.communities:\n",
    "                each_com = list()\n",
    "                for node in com:\n",
    "                    each_com.append(mapping[node])\n",
    "                eva_comm.append(each_com)\n",
    "            \n",
    "            # build a list of labels for the coms for the EVA evaluation df and the user ids df\n",
    "            df_label_coms = []\n",
    "            user_coms = {}\n",
    "            for el in range(len(eva_comm)):\n",
    "                df_label_coms.append(f\"{sem}_{el}\")\n",
    "                for user in eva_comm[el]:\n",
    "                    user_coms[str(user)] = f\"{sem}_{el}\"       \n",
    "                        \n",
    "            df_user_coms = pd.DataFrame(user_coms, index=[\"community\",]).T.rename_axis('user_id').reset_index()\n",
    "            df_user_coms.to_csv(os.path.join(src_results, f\"snapshots/users_community_{topic}_t_{sem}.csv\"),\n",
    "                            index = False)\n",
    "            \n",
    "            # build the df for the EVA evaluation (by semester)\n",
    "            eva_results = pd.DataFrame()\n",
    "            size = evaluation.size(relabel, eva_attr_node_clustering, summary=False)\n",
    "            avg_internal_deg = evaluation.average_internal_degree(relabel, eva_attr_node_clustering, summary=False)\n",
    "            int_edge_dens = evaluation.internal_edge_density(relabel, eva_attr_node_clustering, summary=False)\n",
    "            conductance = evaluation.conductance(relabel, eva_attr_node_clustering, summary=False)\n",
    "            cut_ratio = evaluation.cut_ratio(relabel, eva_attr_node_clustering, summary=False)\n",
    "            link_modularity = evaluation.link_modularity(relabel, eva_attr_node_clustering, summary=False)\n",
    "            edge_inside = evaluation.edges_inside(relabel, eva_attr_node_clustering,summary=False)\n",
    "            purity, max_label = purity_func(coms, labels)\n",
    "            eva_results['purity'] = purity\n",
    "            eva_results[\"max_label\"] = max_label\n",
    "            eva_results['internal_edge_density'] = int_edge_dens\n",
    "            eva_results['average_internal_degree'] = avg_internal_deg\n",
    "            eva_results['conductance'] = conductance\n",
    "            eva_results['cut_ratio'] = cut_ratio\n",
    "            eva_results['edge_inside'] = edge_inside\n",
    "            eva_results['size'] = size\n",
    "            eva_results[\"timestamp\"] = sem\n",
    "            eva_results[\"community\"] = df_label_coms \n",
    "\n",
    "            # save the df for the evaluation (by semester)\n",
    "            eva_results.to_csv(os.path.join(src_results, f\"snapshots/eva_snapshot_{topic}_{sem}_com_stats.csv\"), index = False)\n",
    "            \n",
    "            eva_users_stats = pd.merge(left=eva_results, right = df_user_coms, left_on=\"community\",\n",
    "                                       right_on = \"community\" , how = \"outer\")\n",
    "            \n",
    "            eva_users_stats.to_csv(os.path.join(src_results, f\"{topic}/eva_users_merged_{sem}_com_stats.csv\"), index = False)\n",
    "            ec = eva_results.loc[(eva_results['purity'] >= 0.7) & (eva_results['conductance'] <= 0.5)].copy()\n",
    "            ec.to_csv(os.path.join(src_results, f\"{topic}/EC/EC_{topic}_{sem}.csv\"), index = False)\n",
    "            not_ec = eva_results.loc[((eva_results['purity'] < 0.7) & (eva_results['conductance'] >= 0.5) | \n",
    "                                  (eva_results['purity'] < 0.7) |(eva_results['conductance'] > 0.5)   ) ].copy()\n",
    "            not_ec.to_csv(os.path.join(src_results, f\"{topic}/non_EC/non_EC_{topic}_{sem}.csv\"), index = False)\n",
    "            sem += 1\n",
    "\n",
    "            \n",
    "        matches = tc.community_matching(jaccard, two_sided=False)\n",
    "        matches_dict[topic] = matches\n",
    "        \n",
    "    with open(os.path.join(src_results, f\"matches_dict.pickle\"), 'wb') as r:\n",
    "        pickle.dump(matches_dict, r, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    return matches_dict\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46257db0-a036-47d2-806d-a418a6506f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_df_text_pipeline(cat, ec_val):\n",
    "    data_path = f\"../tesi_cau/topic_comments/\" \n",
    "    for topic in tqdm(categories):\n",
    "        i = 0\n",
    "        for sem in semesters:\n",
    "            print(\"Current iteration:\", i)\n",
    "        # prendiamo il df delle EC\n",
    "            df_EC = pd.read_csv(os.path.join(src_results, f\"{topic}/{ec_val}/{ec_val}_{topic}_{i}.csv\"))\n",
    "        # global community stats + users\n",
    "            df_users_original = pd.read_csv(os.path.join(src_results, f\"{topic}/eva_users_merged_{i}_com_stats.csv\"))\n",
    "            df_users = df_users_original[(df_users_original[\"size\"] >= 20)].copy()\n",
    "            list_ECs = df_EC.community.tolist()\n",
    "        #a questo punto, abbiamo solo gli utenti nelle EC\n",
    "            df_EC_users = df_users[df_users['community'].isin(list_ECs)].copy()\n",
    "            \n",
    "            print(df_EC_users.community.unique())\n",
    "            \n",
    "            df_EC_users[\"EC\"] = ec_val\n",
    "            ec_users = df_EC_users[[\"user_id\", \"EC\", \"max_label\"]].copy()\n",
    "            ec_users.to_csv(os.path.join(src_results, f\"{topic}/{ec_val}_user_TM_{topic}_{i}.csv\"), index = False)\n",
    "            # dobbiamo recuperare i dati da commenti e post\n",
    "            # i commenti sono all'interno di JSON e NON sono stati ripuliti\n",
    "            # i post sono stati sottoposti a una lieve pulizia del testo\n",
    "            comments = list()\n",
    "            list_temp_df = []\n",
    "            period0 = datetime.datetime.strptime(sem[0], \"%d/%m/%Y\").strftime(\"%d-%m-%Y\")\n",
    "            period1 = datetime.datetime.strptime(sem[1], \"%d/%m/%Y\").strftime(\"%d-%m-%Y\")\n",
    "            semester_user_comments = os.path.join(f'../topic_comments/{topic}/{topic}_{period0}_{period1}/')\n",
    "#             json_files = glob.glob(os.path.join(semester_user_comments, '*.json'))\n",
    "#             dfs = [pd.read_json(fn, lines = True) for fn in json_files]\n",
    "            json_files = [pos_json for pos_json in os.listdir(semester_user_comments) if pos_json.endswith('.json')]\n",
    "            for f in json_files:\n",
    "                f = load_json(os.path.join(semester_user_comments, f))\n",
    "                df = pd.json_normalize(f[\"comments\"],max_level=1)\n",
    "                list_temp_df.append(df)\n",
    "\n",
    "            df_text_users = pd.concat(list_temp_df)\n",
    "            print(topic, i)\n",
    "            # per risovere il problema dei commenti nella colonna sbagliata in alcuni dataset\n",
    "            if topic == \"minority\" and i == 4:\n",
    "                df_text_users.clean_text.fillna(df_text_users['body'], inplace=True)\n",
    "                del df_text_users[\"body\"]\n",
    "            elif \"body\" in df_text_users.columns:\n",
    "                df_text_users[\"clean_text\"] = df_text_users[\"body\"]\n",
    "                del df_text_users[\"body\"]\n",
    "                  \n",
    "            df_final_users = pd.merge(left=df_EC_users[[\"community\", \"user_id\"]], right = df_text_users, left_on=\"user_id\", \n",
    "                                      right_on = \"author\" , how = \"inner\")\n",
    "     #       cols_to_delete_comments = \"parent_id\",\"link_id\", \"subreddit_id\", \"date\", \"score\"]\n",
    "        \n",
    "        #    df_final_users.drop(cols_to_delete_comments, axis = 1, inplace = True)  \n",
    "            df_final_users.to_csv(os.path.join(f\"../text_data/comments/{topic}/{ec_val}_comments_{topic}_{i}.csv\"), index = False)\n",
    "            i+=1\n",
    "     # estraiamo quindi un dataframe con la seguente struttura:\n",
    "        # nome utente | subreddit | timestamp | topic | testo | post (booleani) | commento (booleani) | stats "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894adf78-32c4-4ce8-9b5f-aa240936fcb5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80a9f920410f4619a245c949c2fe337e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration: 0\n",
      "['0_1' '0_3' '0_4']\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m preprocess_df_text_pipeline(categories, \u001b[39m\"\u001b[39;49m\u001b[39mEC\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "Cell \u001b[1;32mIn[26], line 34\u001b[0m, in \u001b[0;36mpreprocess_df_text_pipeline\u001b[1;34m(cat, ec_val)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m json_files:\n\u001b[0;32m     33\u001b[0m     f \u001b[39m=\u001b[39m load_json(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(semester_user_comments, f))\n\u001b[1;32m---> 34\u001b[0m     df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mjson_normalize(f[\u001b[39m\"\u001b[39;49m\u001b[39mcomments\u001b[39;49m\u001b[39m\"\u001b[39;49m],max_level\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[0;32m     35\u001b[0m     list_temp_df\u001b[39m.\u001b[39mappend(df)\n\u001b[0;32m     37\u001b[0m df_text_users \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mconcat(list_temp_df)\n",
      "File \u001b[1;32mc:\\Users\\pluez\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\json\\_normalize.py:470\u001b[0m, in \u001b[0;36mjson_normalize\u001b[1;34m(data, record_path, meta, meta_prefix, record_prefix, errors, sep, max_level)\u001b[0m\n\u001b[0;32m    461\u001b[0m \u001b[39mif\u001b[39;00m record_path \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    462\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39many\u001b[39m([\u001b[39misinstance\u001b[39m(x, \u001b[39mdict\u001b[39m) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m y\u001b[39m.\u001b[39mvalues()] \u001b[39mfor\u001b[39;00m y \u001b[39min\u001b[39;00m data):\n\u001b[0;32m    463\u001b[0m         \u001b[39m# naive normalization, this is idempotent for flat records\u001b[39;00m\n\u001b[0;32m    464\u001b[0m         \u001b[39m# and potentially will inflate the data considerably for\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    468\u001b[0m         \u001b[39m# TODO: handle record value which are lists, at least error\u001b[39;00m\n\u001b[0;32m    469\u001b[0m         \u001b[39m#       reasonably\u001b[39;00m\n\u001b[1;32m--> 470\u001b[0m         data \u001b[39m=\u001b[39m nested_to_record(data, sep\u001b[39m=\u001b[39;49msep, max_level\u001b[39m=\u001b[39;49mmax_level)\n\u001b[0;32m    471\u001b[0m     \u001b[39mreturn\u001b[39;00m DataFrame(data)\n\u001b[0;32m    472\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(record_path, \u001b[39mlist\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\pluez\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\json\\_normalize.py:92\u001b[0m, in \u001b[0;36mnested_to_record\u001b[1;34m(ds, prefix, sep, level, max_level)\u001b[0m\n\u001b[0;32m     90\u001b[0m new_ds \u001b[39m=\u001b[39m []\n\u001b[0;32m     91\u001b[0m \u001b[39mfor\u001b[39;00m d \u001b[39min\u001b[39;00m ds:\n\u001b[1;32m---> 92\u001b[0m     new_d \u001b[39m=\u001b[39m copy\u001b[39m.\u001b[39;49mdeepcopy(d)\n\u001b[0;32m     93\u001b[0m     \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m d\u001b[39m.\u001b[39mitems():\n\u001b[0;32m     94\u001b[0m         \u001b[39m# each key gets renamed with prefix\u001b[39;00m\n\u001b[0;32m     95\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(k, \u001b[39mstr\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\pluez\\AppData\\Local\\Programs\\Python\\Python39\\lib\\copy.py:146\u001b[0m, in \u001b[0;36mdeepcopy\u001b[1;34m(x, memo, _nil)\u001b[0m\n\u001b[0;32m    144\u001b[0m copier \u001b[39m=\u001b[39m _deepcopy_dispatch\u001b[39m.\u001b[39mget(\u001b[39mcls\u001b[39m)\n\u001b[0;32m    145\u001b[0m \u001b[39mif\u001b[39;00m copier \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 146\u001b[0m     y \u001b[39m=\u001b[39m copier(x, memo)\n\u001b[0;32m    147\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    148\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39missubclass\u001b[39m(\u001b[39mcls\u001b[39m, \u001b[39mtype\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\pluez\\AppData\\Local\\Programs\\Python\\Python39\\lib\\copy.py:230\u001b[0m, in \u001b[0;36m_deepcopy_dict\u001b[1;34m(x, memo, deepcopy)\u001b[0m\n\u001b[0;32m    228\u001b[0m memo[\u001b[39mid\u001b[39m(x)] \u001b[39m=\u001b[39m y\n\u001b[0;32m    229\u001b[0m \u001b[39mfor\u001b[39;00m key, value \u001b[39min\u001b[39;00m x\u001b[39m.\u001b[39mitems():\n\u001b[1;32m--> 230\u001b[0m     y[deepcopy(key, memo)] \u001b[39m=\u001b[39m deepcopy(value, memo)\n\u001b[0;32m    231\u001b[0m \u001b[39mreturn\u001b[39;00m y\n",
      "File \u001b[1;32mc:\\Users\\pluez\\AppData\\Local\\Programs\\Python\\Python39\\lib\\copy.py:137\u001b[0m, in \u001b[0;36mdeepcopy\u001b[1;34m(x, memo, _nil)\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[39mif\u001b[39;00m memo \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    135\u001b[0m     memo \u001b[39m=\u001b[39m {}\n\u001b[1;32m--> 137\u001b[0m d \u001b[39m=\u001b[39m \u001b[39mid\u001b[39;49m(x)\n\u001b[0;32m    138\u001b[0m y \u001b[39m=\u001b[39m memo\u001b[39m.\u001b[39mget(d, _nil)\n\u001b[0;32m    139\u001b[0m \u001b[39mif\u001b[39;00m y \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m _nil:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "preprocess_df_text_pipeline(categories, \"EC\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4c9747-d4e3-4f07-8fb5-d343f60269f0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'categories' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m preprocess_df_text_pipeline(categories, \u001b[39m\"\u001b[39m\u001b[39mnon_EC\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'categories' is not defined"
     ]
    }
   ],
   "source": [
    "preprocess_df_text_pipeline(categories, \"non_EC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b10498-2984-4b7a-8ef3-4a4b03c5a300",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_post_text_pipeline(cat, ec_val):\n",
    "    data_path = f\"../topic_posts/\" \n",
    "    for topic in tqdm(categories):\n",
    "        i = 0\n",
    "        for sem in semesters:\n",
    "            print(\"Current iteration:\", i)\n",
    "        # prendiamo il df delle EC per vedere l'id delle community EC\n",
    "            df_community = pd.read_csv(os.path.join(src_results, f\"{topic}/{ec_val}/{ec_val}_{topic}_{i}.csv\"))\n",
    "        #prendiamo il df degli utenti nelle community\n",
    "            df_stats = pd.read_csv(os.path.join(src_results, f\"{topic}/eva_users_merged_{i}_com_stats.csv\"))\n",
    "            df_users = df_stats\n",
    "            coms_list = df_community.community.tolist()\n",
    "            \n",
    "            \n",
    "            df_EC_users = df_users[df_users['community'].isin(coms_list)].copy()\n",
    "            ec_users = df_EC_users[[\"user_id\", \"max_label\"]].copy()\n",
    "            # TM users' list\n",
    "            ec_users.to_csv(os.path.join(src_results, f\"{topic}/{ec_val}_user_TM_{topic}_{i}.csv\"), index = False)\n",
    "            \n",
    "            \n",
    "            # i post sono stati sottoposti a una lieve pulizia del testo\n",
    "            period0 = datetime.datetime.strptime(sem[0], \"%d/%m/%Y\").strftime(\"%d-%m-%Y\")\n",
    "            period1 = datetime.datetime.strptime(sem[1], \"%d/%m/%Y\").strftime(\"%d-%m-%Y\")\n",
    "            filename = os.path.join(data_path, f'{topic}/{topic}_{period0}_{period1}.csv')\n",
    "            df_post = pd.read_csv(filename)\n",
    "            df_filtered_post = df_post[df_post[\"author\"].isin(df_EC_users[\"user_id\"].tolist())].copy()\n",
    "            \n",
    "            # author\n",
    "            df_final = pd.merge(left=df_EC_users[[\"community\", \"user_id\"]], right = df_filtered_post, left_on=\"user_id\", \n",
    "                                       right_on = \"author\" , how = \"inner\")\n",
    "\n",
    "           # df_final.drop(cols_to_delete_posts, axis = 1, inplace = True)  \n",
    "            df_final[\"EC_val\"] = ec_val\n",
    "            df_final.to_csv(os.path.join(f\"../text_data/posts/{topic}/{ec_val}_post_{topic}_{i}.csv\"), index = False)\n",
    "            i+=1\n",
    "     # estraiamo quindi un dataframe con la seguente struttura:\n",
    "        # nome utente | subreddit | timestamp | topic | testo | post (booleani) | commento (booleani) | stats "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7428d1d-f6ac-4cbe-83d0-be3589e13c7b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "preprocess_post_text_pipeline(categories, \"EC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbcb009b-79a8-4506-8d55-413fa29313ad",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f28bd30e60e14047ab73506b8015e8af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration: 0\n",
      "              user_id  max_label\n",
      "4575        gatsby137  antitrump\n",
      "4576  wcarterchambers  antitrump\n",
      "4577     FutureAvenir  antitrump\n",
      "4578      rowenkariya  antitrump\n",
      "4579        hjvteffer  antitrump\n",
      "...               ...        ...\n",
      "7848       VirjhinBoy  antitrump\n",
      "7849           hmwith  antitrump\n",
      "7850       lithobolos  antitrump\n",
      "7858   fuckin_bubbles  antitrump\n",
      "7859  VinylAndOctavia  antitrump\n",
      "\n",
      "[1948 rows x 2 columns]\n",
      "Current iteration: 1\n",
      "               user_id  max_label\n",
      "0     Sherlock--Holmes   protrump\n",
      "1             jblack94   protrump\n",
      "2                UDT22   protrump\n",
      "3           mikesteane   protrump\n",
      "4          Taxus_Calyx   protrump\n",
      "...                ...        ...\n",
      "8811          Juyil900   protrump\n",
      "8812      mairedemerde  antitrump\n",
      "8813     Ignatiusloyal    neutral\n",
      "8814      finiksrising    neutral\n",
      "8815    nguyenkhuong92  antitrump\n",
      "\n",
      "[8273 rows x 2 columns]\n",
      "Current iteration: 2\n",
      "                   user_id max_label\n",
      "0               TCDWarrior  protrump\n",
      "1     breadstickcrustybuns  protrump\n",
      "2                DougDante  protrump\n",
      "3        PlastIconoclastic  protrump\n",
      "4                 Dgprehec  protrump\n",
      "...                    ...       ...\n",
      "5556         OzSleeperCell  protrump\n",
      "5557              Wolosocu  protrump\n",
      "5558              dacracot  protrump\n",
      "5563         burn_feminazi  protrump\n",
      "5564        dramaticsystem  protrump\n",
      "\n",
      "[4309 rows x 2 columns]\n",
      "Current iteration: 3\n",
      "                 user_id  max_label\n",
      "0             mikesteane   protrump\n",
      "1              TroLLageK   protrump\n",
      "2               mwobuddy   protrump\n",
      "3           epic_cartman   protrump\n",
      "4               furchfur   protrump\n",
      "...                  ...        ...\n",
      "10273      zimbabwe69420  antitrump\n",
      "10274  PortraitOfSanity5  antitrump\n",
      "10434         peudechose   protrump\n",
      "10435         youfancyme   protrump\n",
      "10436        GreenFrog76   protrump\n",
      "\n",
      "[8123 rows x 2 columns]\n",
      "Current iteration: 4\n",
      "                user_id  max_label\n",
      "168           Fudrucker   protrump\n",
      "169        WhackDanielz   protrump\n",
      "170           5quickdub   protrump\n",
      "171       vaguelydecent   protrump\n",
      "172        VVindowmaker   protrump\n",
      "..                  ...        ...\n",
      "772  fiftytwocardpickup    neutral\n",
      "773       DutchmanDavid   protrump\n",
      "774        Z_for_Zontar   protrump\n",
      "775       UncleThursday  antitrump\n",
      "776     jesusjonesjesus  antitrump\n",
      "\n",
      "[469 rows x 2 columns]\n",
      "Current iteration: 0\n",
      "                  user_id  max_label\n",
      "0        AlexJonesIsMyDad  antitrump\n",
      "1                barawo33  antitrump\n",
      "2             RachaelRay_  antitrump\n",
      "3              forniation  antitrump\n",
      "4            isaacbonyuet  antitrump\n",
      "...                   ...        ...\n",
      "10804         robinson217   protrump\n",
      "10805             adamaoc  antitrump\n",
      "10806        NellyGilbert   protrump\n",
      "10807               Zebid  antitrump\n",
      "10808  ill_silent_lasagna   protrump\n",
      "\n",
      "[10388 rows x 2 columns]\n",
      "Current iteration: 1\n",
      "           user_id  max_label\n",
      "0      cococrispys   protrump\n",
      "1           tubbem   protrump\n",
      "2        Typo-Kign   protrump\n",
      "3         Gerrigen   protrump\n",
      "4         Pirateer   protrump\n",
      "...            ...        ...\n",
      "7827      pisterkk   protrump\n",
      "7829     Y05H1M4U5   protrump\n",
      "7830  usatrump2020  antitrump\n",
      "7831     Crypticox  antitrump\n",
      "7832          FGND   protrump\n",
      "\n",
      "[3646 rows x 2 columns]\n",
      "Current iteration: 2\n",
      "                   user_id  max_label\n",
      "6501        reddit_amnesia  antitrump\n",
      "6502  provocative_pancakes  antitrump\n",
      "6503                 Merk0  antitrump\n",
      "6504           ihavetetnus  antitrump\n",
      "6505              hayek556  antitrump\n",
      "...                    ...        ...\n",
      "6762    checkerboardSlipOn  antitrump\n",
      "6763     all_tea_all_shade   protrump\n",
      "6764             LuxCannon   protrump\n",
      "6765       BoredFatGuy9000   protrump\n",
      "6766   CommanderSmokeStack    neutral\n",
      "\n",
      "[257 rows x 2 columns]\n",
      "Current iteration: 3\n",
      "                 user_id  max_label\n",
      "0       DJ_Baxter_Blaise   protrump\n",
      "1               tehdubya   protrump\n",
      "2           nosoupforyou   protrump\n",
      "3       AlphaTangoFoxtrt   protrump\n",
      "4     WeishauptDeathMask   protrump\n",
      "...                  ...        ...\n",
      "7229          Parker1751  antitrump\n",
      "7230             hego555  antitrump\n",
      "7231         Ibeagoodman  antitrump\n",
      "7235       FISHneedWATER   protrump\n",
      "7236          Tylerc0722   protrump\n",
      "\n",
      "[5899 rows x 2 columns]\n",
      "Current iteration: 4\n",
      "                   user_id  max_label\n",
      "0       Blaine_Harlen_Chet   protrump\n",
      "1             Pariahdog119   protrump\n",
      "2         TheCIASellsDrugs   protrump\n",
      "3                    xvult   protrump\n",
      "4         aussiekangaroo95   protrump\n",
      "...                    ...        ...\n",
      "7657            BlancFranc  antitrump\n",
      "7658             Jonasa113  antitrump\n",
      "7659  TRIGGEREDharambe2018  antitrump\n",
      "7664    TaylorMasonCapital  antitrump\n",
      "7665           darkwolf523  antitrump\n",
      "\n",
      "[7659 rows x 2 columns]\n",
      "Current iteration: 0\n",
      "                user_id  max_label\n",
      "0                 NoFlo   protrump\n",
      "1                tpedes   protrump\n",
      "2           cocksherpa2   protrump\n",
      "3                vvelox   protrump\n",
      "4    StaplerLivesMatter   protrump\n",
      "..                  ...        ...\n",
      "812    smashed_mcdouble  antitrump\n",
      "813          griffinj98   protrump\n",
      "814         captcha_bot   protrump\n",
      "815             XC1729a   protrump\n",
      "816       FormalDissent   protrump\n",
      "\n",
      "[683 rows x 2 columns]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m preprocess_post_text_pipeline(categories, \u001b[39m\"\u001b[39;49m\u001b[39mnon_EC\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "Cell \u001b[1;32mIn[19], line 30\u001b[0m, in \u001b[0;36mpreprocess_post_text_pipeline\u001b[1;34m(cat, ec_val)\u001b[0m\n\u001b[0;32m     27\u001b[0m  df_filtered_post \u001b[39m=\u001b[39m df_post[df_post[\u001b[39m\"\u001b[39m\u001b[39mauthor\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39misin(df_EC_users[\u001b[39m\"\u001b[39m\u001b[39muser_id\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mtolist())]\u001b[39m.\u001b[39mcopy()\n\u001b[0;32m     29\u001b[0m  \u001b[39m# author\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m  df_final \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mmerge(left\u001b[39m=\u001b[39;49mdf_EC_users[[\u001b[39m\"\u001b[39;49m\u001b[39mcommunity\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39muser_id\u001b[39;49m\u001b[39m\"\u001b[39;49m]], right \u001b[39m=\u001b[39;49m df_filtered_post, left_on\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39muser_id\u001b[39;49m\u001b[39m\"\u001b[39;49m, \n\u001b[0;32m     31\u001b[0m                             right_on \u001b[39m=\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mauthor\u001b[39;49m\u001b[39m\"\u001b[39;49m , how \u001b[39m=\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39minner\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m     33\u001b[0m \u001b[39m# df_final.drop(cols_to_delete_posts, axis = 1, inplace = True)  \u001b[39;00m\n\u001b[0;32m     34\u001b[0m  df_final[\u001b[39m\"\u001b[39m\u001b[39mEC_val\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m ec_val\n",
      "File \u001b[1;32mc:\\Users\\pluez\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\reshape\\merge.py:158\u001b[0m, in \u001b[0;36mmerge\u001b[1;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[39m@Substitution\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mleft : DataFrame or named Series\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    128\u001b[0m \u001b[39m@Appender\u001b[39m(_merge_doc, indents\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m    129\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmerge\u001b[39m(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    142\u001b[0m     validate: \u001b[39mstr\u001b[39m \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    143\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame:\n\u001b[0;32m    144\u001b[0m     op \u001b[39m=\u001b[39m _MergeOperation(\n\u001b[0;32m    145\u001b[0m         left,\n\u001b[0;32m    146\u001b[0m         right,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    156\u001b[0m         validate\u001b[39m=\u001b[39mvalidate,\n\u001b[0;32m    157\u001b[0m     )\n\u001b[1;32m--> 158\u001b[0m     \u001b[39mreturn\u001b[39;00m op\u001b[39m.\u001b[39;49mget_result(copy\u001b[39m=\u001b[39;49mcopy)\n",
      "File \u001b[1;32mc:\\Users\\pluez\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\reshape\\merge.py:805\u001b[0m, in \u001b[0;36m_MergeOperation.get_result\u001b[1;34m(self, copy)\u001b[0m\n\u001b[0;32m    802\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindicator:\n\u001b[0;32m    803\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mleft, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mright \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_indicator_pre_merge(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mleft, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mright)\n\u001b[1;32m--> 805\u001b[0m join_index, left_indexer, right_indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_join_info()\n\u001b[0;32m    807\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reindex_and_concat(\n\u001b[0;32m    808\u001b[0m     join_index, left_indexer, right_indexer, copy\u001b[39m=\u001b[39mcopy\n\u001b[0;32m    809\u001b[0m )\n\u001b[0;32m    810\u001b[0m result \u001b[39m=\u001b[39m result\u001b[39m.\u001b[39m__finalize__(\u001b[39mself\u001b[39m, method\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_merge_type)\n",
      "File \u001b[1;32mc:\\Users\\pluez\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\reshape\\merge.py:1053\u001b[0m, in \u001b[0;36m_MergeOperation._get_join_info\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1049\u001b[0m     join_index, right_indexer, left_indexer \u001b[39m=\u001b[39m _left_join_on_index(\n\u001b[0;32m   1050\u001b[0m         right_ax, left_ax, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mright_join_keys, sort\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msort\n\u001b[0;32m   1051\u001b[0m     )\n\u001b[0;32m   1052\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1053\u001b[0m     (left_indexer, right_indexer) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_join_indexers()\n\u001b[0;32m   1055\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mright_index:\n\u001b[0;32m   1056\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mleft) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\pluez\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\reshape\\merge.py:1026\u001b[0m, in \u001b[0;36m_MergeOperation._get_join_indexers\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1024\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_join_indexers\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mtuple\u001b[39m[npt\u001b[39m.\u001b[39mNDArray[np\u001b[39m.\u001b[39mintp], npt\u001b[39m.\u001b[39mNDArray[np\u001b[39m.\u001b[39mintp]]:\n\u001b[0;32m   1025\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"return the join indexers\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1026\u001b[0m     \u001b[39mreturn\u001b[39;00m get_join_indexers(\n\u001b[0;32m   1027\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mleft_join_keys, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mright_join_keys, sort\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msort, how\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhow\n\u001b[0;32m   1028\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\pluez\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\reshape\\merge.py:1672\u001b[0m, in \u001b[0;36mget_join_indexers\u001b[1;34m(left_keys, right_keys, sort, how, **kwargs)\u001b[0m\n\u001b[0;32m   1662\u001b[0m join_func \u001b[39m=\u001b[39m {\n\u001b[0;32m   1663\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39minner\u001b[39m\u001b[39m\"\u001b[39m: libjoin\u001b[39m.\u001b[39minner_join,\n\u001b[0;32m   1664\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mleft\u001b[39m\u001b[39m\"\u001b[39m: libjoin\u001b[39m.\u001b[39mleft_outer_join,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1668\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mouter\u001b[39m\u001b[39m\"\u001b[39m: libjoin\u001b[39m.\u001b[39mfull_outer_join,\n\u001b[0;32m   1669\u001b[0m }[how]\n\u001b[0;32m   1671\u001b[0m \u001b[39m# error: Cannot call function of unknown type\u001b[39;00m\n\u001b[1;32m-> 1672\u001b[0m \u001b[39mreturn\u001b[39;00m join_func(lkey, rkey, count, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "preprocess_post_text_pipeline(categories, \"non_EC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf38ca13-f548-490a-ade8-fd297b48faa9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da445e50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
