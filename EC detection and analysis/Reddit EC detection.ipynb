{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5d2ec163",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Reddit EC detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8d0510",
   "metadata": {},
   "source": [
    "This notebook contains the code to perform Dynamic Community Detection (DCD) and the extraction of echo chambers\n",
    "from the resulting communities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd16e26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "import operator\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "from cdlib import AttrNodeClustering, evaluation, TemporalClustering\n",
    "from Eva import eva_best_partition, modularity, purity\n",
    "from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.lines as mlines\n",
    "sns.set_palette(\"Set2\")\n",
    "\n",
    "sns.set_style(style='white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "473b42ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_json(file):\n",
    "    '''Load a json file'''\n",
    "    with open(file) as f:\n",
    "        json_data = json.loads(f.read())\n",
    "    return json_data\n",
    "\n",
    "def read_net(filename):\n",
    "    '''Read a network from a csv file'''\n",
    "    print(\"\\nEdgelist: \", filename)\n",
    "    g = nx.Graph()\n",
    "    with open(filename) as f:\n",
    "        f.readline()\n",
    "        for l in f:\n",
    "            l = l.split(\",\")\n",
    "            g.add_edge(l[0], l[1], weight=int(l[2]))\n",
    "    return g\n",
    "\n",
    "def read_labels(filename):\n",
    "    '''Read labels from a csv file'''\n",
    "    node_to_label = {}\n",
    "    with open(filename) as f:\n",
    "        f.readline()\n",
    "        for l in f:\n",
    "            l = l.rstrip().split(\",\")\n",
    "            node_to_label[l[0]] = l[2]\n",
    "    return node_to_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cea6450e",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_results = \"../results/reddit_results/\"\n",
    "text_data = \"../text_data/\"\n",
    "\n",
    "categories = [\"guncontrol\", 'minority', \"politics\"]\n",
    "semesters = [('01/01/2017','01/07/2017'), ('01/07/2017','01/01/2018'),\n",
    "             ('01/01/2018','01/07/2018'), ('01/07/2018','01/01/2019'), ('01/01/2019','01/07/2019')]\n",
    "\n",
    "mods = load_json(\"../data/moderators.json\")\n",
    "bots = load_json(\"../data/bots_reddit.json\")\n",
    "\n",
    "mods_bots_list = list(mods.keys())\n",
    "mods_bots_list += list(bots.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0b32d40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_labels(g):\n",
    "    comps_list = list(nx.connected_components(g))\n",
    "    max_len = sorted([[len(el),el] for el in comps_list], reverse=True)\n",
    "    comp_0 = nx.subgraph(g, max_len[0][1])\n",
    "    mapping = dict(zip(comp_0, range(0, len(comp_0))))\n",
    "    relabel_comp_0 = nx.relabel_nodes(comp_0, mapping)\n",
    "    inv_map = {v: k for k, v in mapping.items()}\n",
    "    return relabel_comp_0, inv_map\n",
    "\n",
    "\n",
    "def purity_func(coms, labels):\n",
    "    count_coms = defaultdict(int)\n",
    "    purities = []\n",
    "    top_label = []\n",
    "    for n, c in coms.items():\n",
    "        count_coms[c] += 1\n",
    "    for el in labels: \n",
    "        max_label = max(labels[el][\"leaning\"].items(), key = operator.itemgetter(1))[0]\n",
    "        max_label_val = labels[el][\"leaning\"][max_label]\n",
    "        purity = max_label_val/count_coms[el]\n",
    "        purities.append(purity)\n",
    "        top_label.append(max_label)\n",
    "    return purities, top_label\n",
    "\n",
    "\n",
    "def cd_eva(node_label):\n",
    "    coms, com_labels = eva_best_partition(node_label, alpha=0.5)\n",
    "    return coms, com_labels\n",
    "\n",
    "\n",
    "def extract_EC(categories):\n",
    "    matches_dict = defaultdict(list)\n",
    "    for topic in tqdm(categories):\n",
    "        tc = TemporalClustering()\n",
    "        sem = 0\n",
    "        data_path = f\"../data/topic_networks/{topic}/\"\n",
    "        for semester in semesters:\n",
    "            period0_nodelist= datetime.datetime.strptime(semester[0], \"%d/%m/%Y\").strftime(\"%d-%m-%Y\")\n",
    "            period1_nodelist = datetime.datetime.strptime(semester[1], \"%d/%m/%Y\").strftime(\"%d-%m-%Y\")\n",
    "            period0_labels= datetime.datetime.strptime(semester[0], \"%d/%m/%Y\").strftime(\"%Y-%m-%d\")\n",
    "            period1_labels = datetime.datetime.strptime(semester[1], \"%d/%m/%Y\").strftime(\"%Y-%m-%d\")\n",
    "            semester_edgelist = os.path.join(data_path, f'{topic}_{period0_nodelist}_{period1_nodelist}_complete.csv') \n",
    "            semester_labels = os.path.join(data_path, f'{topic}_{period0_labels}_{period1_labels}_labels.csv') \n",
    "            # read the network and add the attributes\n",
    "            g = read_net(semester_edgelist)\n",
    "            nth = read_labels(semester_labels) \n",
    "            nx.set_node_attributes(g, nth, \"leaning\")\n",
    "            nx.set_node_attributes(g, sem, \"snapshot_id\")\n",
    "            \n",
    "            # remove nodes without attributes, mods and bots from the network\n",
    "            nodes_to_remove = list()\n",
    "            for node in g.nodes:\n",
    "                node_dict = g.nodes[node]\n",
    "                if node_dict.get('leaning') is None:\n",
    "                    nodes_to_remove.append(node)\n",
    "                if node in mods_bots_list:\n",
    "                    nodes_to_remove.append(node)\n",
    "            g.remove_nodes_from(nodes_to_remove)\n",
    "\n",
    "            relabel, mapping = map_labels(g)\n",
    "            # community detection with EVA\n",
    "            coms, labels = cd_eva(relabel)\n",
    "            coms_to_node = defaultdict(list)\n",
    "            for n, c in coms.items():\n",
    "                coms_to_node[c].append(n) \n",
    "            coms_eva = [list(c) for c in coms_to_node.values()]\n",
    "            eva_attr_node_clustering = AttrNodeClustering(coms_eva, relabel, \"Eva\", labels, method_parameters={\"weight\": 'weight', \"resolution\": 1,\n",
    "                                                                             \"randomize\": False, \"alpha\":0.5})\n",
    "            tc.add_clustering(eva_attr_node_clustering, sem)\n",
    "            eva_comm = list()\n",
    "            for com in eva_attr_node_clustering.communities:\n",
    "                each_com = list()\n",
    "                for node in com:\n",
    "                    each_com.append(mapping[node])\n",
    "                eva_comm.append(each_com)\n",
    "            \n",
    "            # build a list of labels for the coms for the EVA evaluation df and the user ids df\n",
    "            df_label_coms = []\n",
    "            user_coms = {}\n",
    "            for el in range(len(eva_comm)):\n",
    "                df_label_coms.append(f\"{sem}_{el}\")\n",
    "                for user in eva_comm[el]:\n",
    "                    user_coms[str(user)] = f\"{sem}_{el}\"       \n",
    "                        \n",
    "            df_user_coms = pd.DataFrame(user_coms, index=[\"community\",]).T.rename_axis('user_id').reset_index()\n",
    "            df_user_coms.to_csv(os.path.join(src_results, f\"snapshots/users_community_{topic}_t_{sem}.csv\"),\n",
    "                            index = False)\n",
    "            \n",
    "            # build the df for the EVA evaluation (by semester)\n",
    "            eva_results = pd.DataFrame()\n",
    "            size = evaluation.size(relabel, eva_attr_node_clustering, summary=False)\n",
    "            avg_internal_deg = evaluation.average_internal_degree(relabel, eva_attr_node_clustering, summary=False)\n",
    "            int_edge_dens = evaluation.internal_edge_density(relabel, eva_attr_node_clustering, summary=False)\n",
    "            conductance = evaluation.conductance(relabel, eva_attr_node_clustering, summary=False)\n",
    "            cut_ratio = evaluation.cut_ratio(relabel, eva_attr_node_clustering, summary=False)\n",
    "            link_modularity = evaluation.link_modularity(relabel, eva_attr_node_clustering, summary=False)\n",
    "            edge_inside = evaluation.edges_inside(relabel, eva_attr_node_clustering,summary=False)\n",
    "            purity, max_label = purity_func(coms, labels)\n",
    "            eva_results['purity'] = purity\n",
    "            eva_results[\"max_label\"] = max_label\n",
    "            eva_results['internal_edge_density'] = int_edge_dens\n",
    "            eva_results['average_internal_degree'] = avg_internal_deg\n",
    "            eva_results['conductance'] = conductance\n",
    "            eva_results['cut_ratio'] = cut_ratio\n",
    "            eva_results['edge_inside'] = edge_inside\n",
    "            eva_results['size'] = size\n",
    "            eva_results[\"timestamp\"] = sem\n",
    "            eva_results[\"community\"] = df_label_coms \n",
    "\n",
    "            # save the df for the evaluation (by semester)\n",
    "            eva_results.to_csv(os.path.join(src_results, f\"snapshots/eva_snapshot_{topic}_{sem}_com_stats.csv\"), index = False)\n",
    "            \n",
    "            eva_users_stats = pd.merge(left=eva_results, right = df_user_coms, left_on=\"community\",\n",
    "                                       right_on = \"community\" , how = \"outer\")\n",
    "            \n",
    "            eva_users_stats.to_csv(os.path.join(src_results, f\"{topic}/eva_users_merged_{sem}_com_stats.csv\"), index = False)\n",
    "            ec = eva_results.loc[(eva_results['purity'] >= 0.7) & (eva_results['conductance'] <= 0.5)].copy()\n",
    "            ec.to_csv(os.path.join(src_results, f\"{topic}/EC/EC_{topic}_{sem}.csv\"), index = False)\n",
    "            not_ec = eva_results.loc[((eva_results['purity'] < 0.7) & (eva_results['conductance'] >= 0.5) | \n",
    "                                  (eva_results['purity'] < 0.7) |(eva_results['conductance'] > 0.5)   ) ].copy()\n",
    "            not_ec.to_csv(os.path.join(src_results, f\"{topic}/non_EC/non_EC_{topic}_{sem}.csv\"), index = False)\n",
    "            sem += 1\n",
    "\n",
    "            \n",
    "        matches = tc.community_matching(jaccard, two_sided=False)\n",
    "        matches_dict[topic] = matches\n",
    "        \n",
    "    with open(os.path.join(src_results, f\"matches_dict.pickle\"), 'wb') as r:\n",
    "        pickle.dump(matches_dict, r, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    return matches_dict\n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
