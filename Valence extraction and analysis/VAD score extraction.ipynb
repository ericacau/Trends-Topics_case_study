{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bfbbc985",
   "metadata": {},
   "source": [
    "# Valence analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-22T13:14:58.708817700Z",
     "start_time": "2023-05-22T13:14:55.235826900Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "import json\n",
    "import pickle\n",
    "from keybert import KeyBERT\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from datetime import datetime\n",
    "from datetime import timezone\n",
    "from itertools import chain\n",
    "import itertools\n",
    "\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "from nrclex import NRCLex\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pyplutchik import plutchik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce966344",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"../models/topic_modeling_posts.pickle\", 'rb') as input_file:\n",
    "    topic_modeling_ec = pd.read_pickle(input_file)\n",
    "\n",
    "with open('../data/VAD_Lexicon_Valence.json') as json_file:\n",
    "    lexicon = json.load(json_file)\n",
    "\n",
    "with open('../data/acronym_list.json') as json_file:\n",
    "    acronym_list = json.load(json_file)\n",
    "\n",
    "with open('../data/contractions_dict.json') as json_file:\n",
    "    contractions_dict = json.load(json_file)\n",
    "\n",
    "topics = [\"minority\", \"politics\", \"guncontrol\"]\n",
    "\n",
    "stopwords = pd.read_csv(\"stop_words_eng.csv\", header = 0, names = [\"stops\"])\n",
    "\n",
    "stopwords_list = stopwords.stops.to_list()\n",
    "stopwords_list += [\"andnbsp\", \"wanna\", \"didn t\", \"didnt\"]\n",
    "\n",
    "src_results = \"/results/reddit_results/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e53b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_vocabulary(text_list):\n",
    "    vocabulary_list = []\n",
    "    kw_model = KeyBERT()\n",
    "    for el in text_list:\n",
    "        keyword_list = []\n",
    "        keywords = kw_model.extract_keywords(el)\n",
    "        if len(keywords) > 0:  \n",
    "            for key in keywords:\n",
    "                k = lemmatizer(key[0])\n",
    "                keyword_list.append(k)\n",
    "        vocabulary_list.append(keyword_list)\n",
    "    return vocabulary_list\n",
    "    \n",
    "def lemmatizer(text):\n",
    "    tokenization = nltk.word_tokenize(text)\n",
    "    lemmatized_sent = \" \".join([wordnet_lemmatizer.lemmatize(w) for w in tokenization])\n",
    "    lemmatized_sent = \" \".join(w for w in lemmatized_sent.split() if len(w)>1)\n",
    "    return lemmatized_sent\n",
    "\n",
    "def extract_valence(lexicon, text):\n",
    "    word_list = [word for word in text.split(\" \") if word not in (stopwords_list)]\n",
    "    valence = 0\n",
    "    n_word = 0\n",
    "    avg_valence = 0\n",
    "    for word in word_list:\n",
    "        if word in acronym_list:\n",
    "            word = acronym_list[word]\n",
    "        if word in lexicon: \n",
    "            valence += lexicon[word]\n",
    "            n_word += 1\n",
    "    if (valence > 0) and (n_word > 0):\n",
    "        avg_valence = valence/n_word\n",
    "    return avg_valence\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb03380",
   "metadata": {},
   "outputs": [],
   "source": [
    "def valence_analysis():\n",
    "    for t in topics:\n",
    "        df_post = topic_modeling_ec[t]\n",
    "        print(\"Columns: \", df_post.columns)\n",
    "        df_post['labels'] = df_post['labels'].str.replace('-',' ')\n",
    "        print(\"Extracting vocabulary\")\n",
    "        df_post = df_post[df_post.clean_text != '']\n",
    "\n",
    "        clean_text_list = df_post.clean_text.values\n",
    "        clean_text_list = [[el] for el in clean_text_list]\n",
    "        \n",
    "        df_post['Vocabulary'] = [' '.join(map(str, l)) for l in df_post['Vocabulary']]\n",
    "\n",
    "        # print(f\"Records in the dataset {t}: \", len(df_post))\n",
    "        df_plot = df_post.groupby([\"EC_val\", \"labels\"]).agg(clean_text = (\"clean_text\", \" \".join),\n",
    "                                                             Volume=(\"labels\", \"size\")).reset_index()\n",
    "        \n",
    "        vocabulary = extract_vocabulary(clean_text)\n",
    "        df_post[\"Vocabulary\"] = vocabulary\n",
    "    #   df_plot['Lemmatized'] = df_plot .Vocabulary.apply(lambda x: lemmatizer(x))\n",
    "\n",
    "        df_plot[\"Valence\"] = df_plot[\"Vocabulary\"].map(lambda x: extract_valence(lexicon, x))\n",
    "\n",
    "        df_plot = df_plot.rename({\"EC_val\": \"Cluster\"})\n",
    "        df_plot.to_csv(f\"valence_post_{t}.csv\", index = False)\n",
    "            \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2270584",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = topic_modeling_ec[\"minority\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
